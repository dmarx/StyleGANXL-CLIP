{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmarx/StyleGANXL-CLIP/blob/main/StyleganXL%2BCLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K74zxktiQyub"
      },
      "source": [
        "# **StyleGANXL + CLIP ðŸ–¼ï¸**: SlightlyStable edition via DigThatData\n",
        "\n",
        "## Generate images from text prompts using StyleGANXL with CLIP guidance.\n",
        "\n",
        "(Modified by Katherine Crowson to optimize in W+ space)\n",
        "\n",
        "This notebook is a work in progress, head over [here](https://github.com/CasualGANPapers/unconditional-StyleGAN-CLIP) if you want to be up to date with its changes.\n",
        "\n",
        "Largely based on code by  [Katherine Crowson](https://github.com/crowsonkb) and [nshepperd](https://github.com/nshepperd).\n",
        "\n",
        "Mostly made possible because of [StyleGAN-XL](https://github.com/autonomousvision/stylegan_xl) and [CLIP](https://github.com/openai/CLIP).\n",
        "\n",
        "Created by [Eugenio Herrera](https://github.com/ouhenio) and [Rodrigo Mello](https://github.com/ryudrigo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i6Ri2kT3N3Gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58af382c-dce8-4692-f587-90cad52e6f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan_xl'...\n",
            "remote: Enumerating objects: 298, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 298 (delta 75), reused 58 (delta 44), pack-reused 192\u001b[K\n",
            "Receiving objects: 100% (298/298), 13.89 MiB | 21.00 MiB/s, done.\n",
            "Resolving deltas: 100% (134/134), done.\n",
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 229, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 229 (delta 1), reused 1 (delta 0), pack-reused 222\u001b[K\n",
            "Receiving objects: 100% (229/229), 8.92 MiB | 21.00 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "Cloning into 'esgd'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 29 (delta 10), reused 27 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (29/29), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/CLIP\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.0+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Installing collected packages: ftfy, clip\n",
            "  Running setup.py develop for clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108 kB 18.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: ninja, einops\n",
            "Successfully installed einops-0.4.1 ninja-1.10.2.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.5-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 512 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.6.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device: cuda:0\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 335M/335M [00:04<00:00, 84.1MiB/s]\n"
          ]
        }
      ],
      "source": [
        "#@markdown #**Install libraries and define functions** ðŸ—ï¸ðŸ› ï¸\n",
        "# @markdown This cell will take a little while because it has to download several libraries.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!git clone https://github.com/autonomousvision/stylegan_xl\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/crowsonkb/esgd.git\n",
        "!pip install -e ./CLIP\n",
        "!pip install einops ninja\n",
        "!pip install timm\n",
        "\n",
        "## I'll probably have to trim stuff here\n",
        "\n",
        "import sys\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./stylegan_xl')\n",
        "sys.path.append('./esgd')\n",
        "\n",
        "import io\n",
        "import os, time, glob\n",
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import clip\n",
        "import unicodedata\n",
        "import re\n",
        "from esgd import ESGD\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange\n",
        "from google.colab import files\n",
        "import dnnlib\n",
        "import legacy\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)\n",
        "\n",
        "# Functions (many must be trimmed too)\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    !wget -c '{url_or_path}'\n",
        "\n",
        "def slugify(value, allow_unicode=False):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
        "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
        "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
        "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
        "    trailing whitespace, dashes, and underscores.\n",
        "    \"\"\"\n",
        "    value = str(value)\n",
        "    if allow_unicode:\n",
        "        value = unicodedata.normalize('NFKC', value)\n",
        "    else:\n",
        "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
        "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
        "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
        "\n",
        "def norm1(prompt):\n",
        "    \"Normalize to the unit sphere.\"\n",
        "    return prompt / prompt.square().sum(dim=-1,keepdim=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "def prompts_dist_loss(x, targets, loss):\n",
        "    if len(targets) == 1: # Keeps consitent results vs previous method for single objective guidance \n",
        "      return loss(x, targets[0])\n",
        "    distances = [loss(x, target) for target in targets]\n",
        "    return torch.stack(distances, dim=-1).sum(dim=-1)  \n",
        "\n",
        "class MakeCutouts(torch.nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "make_cutouts = MakeCutouts(224, 32, 0.5)\n",
        "\n",
        "def embed_image(image):\n",
        "  n = image.shape[0]\n",
        "  cutouts = make_cutouts(image)\n",
        "  embeds = clip_model.embed_cutout(cutouts)\n",
        "  embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
        "  return embeds\n",
        "\n",
        "def embed_url(url):\n",
        "  image = Image.open(fetch(url)).convert('RGB')\n",
        "  return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
        "\n",
        "class CLIP(object):\n",
        "  def __init__(self):\n",
        "    clip_model = \"ViT-B/16\"\n",
        "    self.model, _ = clip.load(clip_model)\n",
        "    self.model = self.model.requires_grad_(False)\n",
        "    self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                          std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def embed_text(self, prompt):\n",
        "      \"Normalized clip text embedding.\"\n",
        "      return norm1(self.model.encode_text(clip.tokenize(prompt).to(device)).float())\n",
        "\n",
        "  def embed_cutout(self, image):\n",
        "      \"Normalized clip image embedding.\"\n",
        "      return norm1(self.model.encode_image(self.normalize(image)))\n",
        "  \n",
        "clip_model = CLIP()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# just use the machinery in pytti for now\n",
        "# super bloated installation relative to what we need\n",
        "\n",
        "\n",
        "# { display-mode: 'form' }\n",
        "\n",
        "\n",
        "## 1. Install stuff\n",
        "\n",
        "try: \n",
        "    import pytti\n",
        "except ImportError:\n",
        "    !pip install kornia pytorch-lightning transformers\n",
        "    !pip install jupyter loguru einops PyGLM ftfy regex tqdm hydra-core exrex\n",
        "    !pip install seaborn adjustText bunch matplotlib-label-lines\n",
        "    !pip install --upgrade gdown\n",
        "\n",
        "    !pip install --upgrade git+https://github.com/pytti-tools/AdaBins.git\n",
        "    !pip install --upgrade git+https://github.com/pytti-tools/GMA.git\n",
        "    !pip install --upgrade git+https://github.com/pytti-tools/taming-transformers.git\n",
        "    !pip install --upgrade git+https://github.com/openai/CLIP.git\n",
        "    !pip install --upgrade git+https://github.com/pytti-tools/pytti-core.git\n",
        "\n",
        "    # These are notebook specific\n",
        "    !pip install --upgrade natsort\n",
        "\n",
        "try:\n",
        "    import mmc\n",
        "except:\n",
        "    # install mmc\n",
        "    !git clone https://github.com/dmarx/Multi-Modal-Comparators\n",
        "    !pip install poetry\n",
        "    !cd Multi-Modal-Comparators; poetry build\n",
        "    !cd Multi-Modal-Comparators; pip install dist/mmc*.whl\n",
        "    !python Multi-Modal-Comparators/src/mmc/napm_installs/__init__.py\n",
        "\n",
        "from natsort import natsorted\n",
        "from omegaconf import OmegaConf\n",
        "from pathlib import Path\n",
        "\n",
        "import mmc.loaders\n",
        "!python -m pytti.warmup\n",
        "\n",
        "notebook_params = {}\n",
        "\n",
        "def get_output_paths():\n",
        "    outv = [str(p.resolve()) for p in Path('outputs/').glob('**/*.png')]\n",
        "    #outv.sort()\n",
        "    outv = natsorted(outv)\n",
        "    return outv"
      ],
      "metadata": {
        "id": "omr1twEbWG5m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "\n",
        "from pytti.LossAug.OpticalFlowLossClass import (\n",
        "    init_GMA,\n",
        "    OpticalFlowLoss,\n",
        ")\n",
        "\n",
        "init_GMA() \n",
        "\n",
        "#flow_forward = OpticalFlowLoss.get_flow(image1, image2, device=device)\n",
        "#flow_backward = OpticalFlowLoss.get_flow(image2, image1, device=device)\n",
        "\n",
        "#fancy_mask = OpticalFlowLoss.motion_edge_map(\n",
        "#    flow_forward, flow_backward, \n",
        "#    # this function doesn't do anythign with any of these three arguments\n",
        "#    img=None,\n",
        "#    border_mode=None,\n",
        "#    sampling_mode=None,\n",
        "#)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def flow_to_image(flow):\n",
        "    \"\"\"\n",
        "    Convert flow into middlebury color code image\n",
        "    :param flow: optical flow map\n",
        "    :return: optical flow image in middlebury color\n",
        "    \"\"\"\n",
        "    UNKNOWN_FLOW_THRESH = 1e7\n",
        "\n",
        "    u = flow[:, :, 0]\n",
        "    v = flow[:, :, 1]\n",
        "\n",
        "    maxu = -999.0\n",
        "    maxv = -999.0\n",
        "    minu = 999.0\n",
        "    minv = 999.0\n",
        "\n",
        "    idxUnknow = (abs(u) > UNKNOWN_FLOW_THRESH) | (abs(v) > UNKNOWN_FLOW_THRESH)\n",
        "    u[idxUnknow] = 0\n",
        "    v[idxUnknow] = 0\n",
        "\n",
        "    maxu = max(maxu, np.max(u))\n",
        "    minu = min(minu, np.min(u))\n",
        "\n",
        "    maxv = max(maxv, np.max(v))\n",
        "    minv = min(minv, np.min(v))\n",
        "\n",
        "    rad = np.sqrt(u**2 + v**2)\n",
        "    maxrad = max(-1, np.max(rad))\n",
        "\n",
        "    u = u / (maxrad + np.finfo(float).eps)\n",
        "    v = v / (maxrad + np.finfo(float).eps)\n",
        "\n",
        "    img = compute_color(u, v)\n",
        "\n",
        "    idx = np.repeat(idxUnknow[:, :, np.newaxis], 3, axis=2)\n",
        "    img[idx] = 0\n",
        "\n",
        "    return np.uint8(img)\n",
        "\n",
        "\n",
        "def compute_color(u, v):\n",
        "    \"\"\"\n",
        "    compute optical flow color map\n",
        "    :param u: optical flow horizontal map\n",
        "    :param v: optical flow vertical map\n",
        "    :return: optical flow in color code\n",
        "    \"\"\"\n",
        "    [h, w] = u.shape\n",
        "    img = np.zeros([h, w, 3])\n",
        "    nanIdx = np.isnan(u) | np.isnan(v)\n",
        "    u[nanIdx] = 0\n",
        "    v[nanIdx] = 0\n",
        "\n",
        "    colorwheel = make_color_wheel()\n",
        "    ncols = np.size(colorwheel, 0)\n",
        "\n",
        "    rad = np.sqrt(u**2 + v**2)\n",
        "\n",
        "    a = np.arctan2(-v, -u) / np.pi\n",
        "\n",
        "    fk = (a + 1) / 2 * (ncols - 1) + 1\n",
        "\n",
        "    k0 = np.floor(fk).astype(int)\n",
        "\n",
        "    k1 = k0 + 1\n",
        "    k1[k1 == ncols + 1] = 1\n",
        "    f = fk - k0\n",
        "\n",
        "    for i in range(0, np.size(colorwheel, 1)):\n",
        "        tmp = colorwheel[:, i]\n",
        "        col0 = tmp[k0 - 1] / 255\n",
        "        col1 = tmp[k1 - 1] / 255\n",
        "        col = (1 - f) * col0 + f * col1\n",
        "\n",
        "        idx = rad <= 1\n",
        "        col[idx] = 1 - rad[idx] * (1 - col[idx])\n",
        "        notidx = np.logical_not(idx)\n",
        "\n",
        "        col[notidx] *= 0.75\n",
        "        img[:, :, i] = np.uint8(np.floor(255 * col * (1 - nanIdx)))\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def make_color_wheel():\n",
        "    \"\"\"\n",
        "    Generate color wheel according Middlebury color code\n",
        "    :return: Color wheel\n",
        "    \"\"\"\n",
        "    RY = 15\n",
        "    YG = 6\n",
        "    GC = 4\n",
        "    CB = 11\n",
        "    BM = 13\n",
        "    MR = 6\n",
        "\n",
        "    ncols = RY + YG + GC + CB + BM + MR\n",
        "\n",
        "    colorwheel = np.zeros([ncols, 3])\n",
        "\n",
        "    col = 0\n",
        "    # RY\n",
        "    colorwheel[0:RY, 0] = 255\n",
        "    colorwheel[0:RY, 1] = np.transpose(np.floor(255 * np.arange(0, RY) / RY))\n",
        "    col += RY\n",
        "    # YG\n",
        "    colorwheel[col : col + YG, 0] = 255 - np.transpose(np.floor(255 * np.arange(0, YG) / YG))\n",
        "    colorwheel[col : col + YG, 1] = 255\n",
        "    col += YG\n",
        "    # GC\n",
        "    colorwheel[col : col + GC, 1] = 255\n",
        "    colorwheel[col : col + GC, 2] = np.transpose(np.floor(255 * np.arange(0, GC) / GC))\n",
        "    col += GC\n",
        "    # CB\n",
        "    colorwheel[col : col + CB, 1] = 255 - np.transpose(np.floor(255 * np.arange(0, CB) / CB))\n",
        "    colorwheel[col : col + CB, 2] = 255\n",
        "    col += CB\n",
        "    # BM\n",
        "    colorwheel[col : col + BM, 2] = 255\n",
        "    colorwheel[col : col + BM, 0] = np.transpose(np.floor(255 * np.arange(0, BM) / BM))\n",
        "    col += +BM\n",
        "    # MR\n",
        "    colorwheel[col : col + MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n",
        "    colorwheel[col : col + MR, 0] = 255\n",
        "\n",
        "    return colorwheel\n",
        "\n",
        "def flow2pil(flow):\n",
        "  im = transforms.ToPILImage()(flow).convert(\"RGB\")\n",
        "  return im"
      ],
      "metadata": {
        "id": "0Nu95yDuWYzf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**Optional:** Save images in Google Drive ðŸ’¾\n",
        "# @markdown Run this cell if you want to store the results inside Google Drive.\n",
        "\n",
        "# @markdown Copying the generated images to drive is faster to work with.\n",
        "\n",
        "# @markdown **Important**: you must have a folder named *samples* inside your drive, otherwise this may not work.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n",
        "\n",
        "mount_drive = False # @param {type:'boolean'}\n",
        "if mount_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JhTuiOf4mUuk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "guZeRGM6OmaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16bb2dfa-c3eb-4549-81af-9e3fa9569ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-26 23:32:57--  https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet1024.pkl\n",
            "Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 52.219.171.133\n",
            "Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|52.219.171.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@markdown #**Model selection** ðŸŽ­\n",
        "\n",
        "Model = 'Imagenet-1024' #@param [\"Imagenet-1024\", \"Imagenet-512\", \"Imagenet-256\", \"Imagenet-128\", \"Pokemon\", \"FFHQ\"]\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "network_url = {\n",
        "    \"Imagenet-1024\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet1024.pkl\",\n",
        "    \"Imagenet-512\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet512.pkl\",\n",
        "    \"Imagenet-256\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet256.pkl\",\n",
        "    \"Imagenet-128\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet128.pkl\",\n",
        "    \"Pokemon-1024\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/pokemon1024.pkl\",\n",
        "    \"Pokemon-512\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/pokemon512.pkl\",\n",
        "    \"Pokemon-256\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/pokemon256.pkl\",\n",
        "    \"FFHQ-256\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/ffhq256.pkl\"\n",
        "}\n",
        "\n",
        "network_name = network_url[Model].split(\"/\")[-1]\n",
        "fetch_model(network_url[Model])\n",
        "\n",
        "with dnnlib.util.open_url(network_name) as f:\n",
        "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
        "\n",
        "\n",
        "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
        "cs = torch.zeros([10000, G.mapping.c_dim], device=device)\n",
        "for i in range(cs.shape[0]):\n",
        "  cs[i,i//10]=1\n",
        "w_stds = G.mapping(zs, cs)\n",
        "w_stds = w_stds.reshape(10, 1000, G.num_ws, -1)\n",
        "w_stds=w_stds.std(0).mean(0)[0]\n",
        "w_all_classes_avg = G.mapping.w_avg.mean(0)\n",
        "\n",
        "aggressive_stabilization = False  # @param {type:'boolean'}\n",
        "\n",
        "dejiggle_weight=100 # @param {type:'number'}\n",
        "\n",
        "ema_decay = 0.5 # @param {type:'number'}\n",
        "\n",
        "if aggressive_stabilization:\n",
        "  dejiggle_weight = 0\n",
        "\n",
        "#####################################################\n",
        "# stabilization snippet via @PDillis\n",
        "# https://twitter.com/PDillis/status/1551673599878209544\n",
        "if hasattr(G.synthesis, 'input') and aggressive_stabilization:\n",
        "    if G.c_dim == 0:\n",
        "        shift = (G\n",
        "            .synthesis\n",
        "            .input\n",
        "            .affine(G.mapping.w_avg.unsqueeze(0))\n",
        "            .squeeze(0)\n",
        "        )\n",
        "    else:\n",
        "        shift = (G\n",
        "            .synthesis\n",
        "            .input\n",
        "            .affine(G.mapping.w_avg)\n",
        "            .mean(0)\n",
        "        )\n",
        "    G.synthesis.input.affine.bias.data.add_(shift)\n",
        "    G.synthesis.input.affine.weight.data.zero_()\n",
        "#####################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAknegrdPM-d",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Run the model** ðŸš€\n",
        "#@markdown `texts`: Enter here a prompt to guide the image generation. You can enter more than one prompt separated with\n",
        "#@markdown `|`, which will cause the guidance to focus on the different prompts at the same time, allowing to mix and play\n",
        "#@markdown with the generation process.\n",
        "\n",
        "#@markdown `steps`: Number of optimization steps. The more steps, the longer it will try to generate an image relevant to the prompt.\n",
        "\n",
        "#@markdown `seed`: Determines the randomness seed. Using the same seed and prompt should give you similar results at every run.\n",
        "#@markdown Use `-1` for a random seed.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "import einops as ei\n",
        "\n",
        "\n",
        "#texts = \"the loom of the gods\" #@param {type:\"string\"}\n",
        "texts = \"steampunk laser dance party\" #@param {type:\"string\"}\n",
        "steps = 360 #@param {type:\"number\"}\n",
        "warmup_steps = 50\n",
        "seed = 12345#@param {type:\"number\"}\n",
        "save_every = 2 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "if seed == -1:\n",
        "    seed = np.random.randint(0,9e9)\n",
        "    print(f\"Your random seed is: {seed}\")\n",
        "\n",
        "texts = [frase.strip() for frase in texts.split(\"|\") if frase]\n",
        "\n",
        "targets = [clip_model.embed_text(text) for text in texts]\n",
        "\n",
        "\n",
        "'''\n",
        "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
        "one_hot_class = torch.zeros(1000)\n",
        "initial_class = torch.randint(0,1000, (1,))[0]\n",
        "one_hot_class[initial_class]=1\n",
        "one_hot_class = one_hot_class.repeat((10000, 1))\n",
        "cs = one_hot_class.to(device)\n",
        "w_stds = G.mapping(zs, cs)\n",
        "w_stds=w_stds.std(0)[0]\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tf = Compose([\n",
        "  Resize(224),\n",
        "  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
        "])\n",
        "\n",
        "initial_batch=4 #actually that will be multiplied by initial_image_steps\n",
        "initial_image_steps=warmup_steps\n",
        "'''\n",
        "c = torch.zeros((1000)) #just to pick a closer initial image\n",
        "c[initial_class]=1\n",
        "c = c.repeat(initial_batch, 1)\n",
        "c=c.to(device)\n",
        "'''\n",
        "def run(timestring):\n",
        "  flow=None\n",
        "  torch.manual_seed(seed)\n",
        "  with torch.no_grad():\n",
        "    qs = []\n",
        "    losses = []\n",
        "    for _ in range(initial_image_steps):\n",
        "      a = torch.randn([initial_batch, 512], device=device)*0.6 + w_all_classes_avg*0.4\n",
        "      q = ((a-w_all_classes_avg)/w_stds)\n",
        "      images = G.synthesis((q * w_stds + w_all_classes_avg).unsqueeze(1).repeat([1, G.num_ws, 1]))\n",
        "      embeds = embed_image(images.add(1).div(2))\n",
        "      loss = prompts_dist_loss(embeds, targets, spherical_dist_loss).mean(0)\n",
        "      i = torch.argmin(loss)\n",
        "      qs.append(q[i])\n",
        "      losses.append(loss[i])\n",
        "    qs = torch.stack(qs)\n",
        "    losses = torch.stack(losses)\n",
        "    i = torch.argmin(losses)\n",
        "    q = qs[i].unsqueeze(0).repeat([G.num_ws, 1]).requires_grad_()\n",
        "    # print(q)\n",
        "    # q = qs[i].unsqueeze(0).repeat([G.num_ws, 1])\n",
        "    # q = torch.tensor(q, requires_grad=True)\n",
        "    # print(q)\n",
        "\n",
        "  im_prev=None\n",
        "  # Sampling loop\n",
        "  q_ema = q\n",
        "  # print(q.shape)\n",
        "  # opt = ESGD([q], lr=0.03, betas=(0.9, 0.999), nu=0.9)\n",
        "  opt = torch.optim.AdamW([q], lr=0.03, betas=(0., 0.999), weight_decay=0.025)\n",
        "  loop = tqdm(range(steps))\n",
        "  j = 0\n",
        "  for i in loop:\n",
        "    opt.zero_grad()\n",
        "\n",
        "    w = q * w_stds\n",
        "    image = G.synthesis((q * w_stds + w_all_classes_avg)[None], noise_mode='const')\n",
        "    embed = embed_image(image.add(1).div(2))\n",
        "    loss = prompts_dist_loss(embed, targets, spherical_dist_loss).mean()\n",
        "\n",
        "    ##############\n",
        "    # de-jiggle loss\n",
        "    if (im_prev is not None) and (dejiggle_weight != 0):\n",
        "        flow = OpticalFlowLoss.get_flow(im_prev, image)\n",
        "        net_flow = flow.mean()\n",
        "        loss += dejiggle_weight * net_flow\n",
        "\n",
        "    ##############\n",
        "    # loss.backward(create_graph=opt.should_create_graph())\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    loop.set_postfix(loss=loss.item(), q_magnitude=q.std().item())\n",
        "\n",
        "    q_ema = q_ema * (1-ema_decay) + q * ema_decay\n",
        "    image = G.synthesis((q_ema * w_stds + w_all_classes_avg)[None],\n",
        "                        noise_mode='const',                        \n",
        "                        )\n",
        "\n",
        "    if i % save_every == 0:\n",
        "      with torch.no_grad():\n",
        "        im_prev = image.clone().detach()\n",
        "      if i % 10 == 0 or i == (steps - 1):\n",
        "        display(TF.to_pil_image(tf(image)[0]))\n",
        "        print(f\"Image {j}, steps {i}/{steps} | Current loss: {loss}\")\n",
        "        #####################\n",
        "        #if flow is not None:\n",
        "        #  im_t_flow_fwd = ei.rearrange(flow, 'b c h w -> b h w c')\n",
        "        #  arr_flow_fwd = flow_to_image(im_t_flow_fwd.squeeze().detach().clone().cpu().numpy())\n",
        "        #  display(flow2pil(arr_flow_fwd))\n",
        "\n",
        "      pil_image = TF.to_pil_image(image[0].add(1).div(2).clamp(0,1))\n",
        "      os.makedirs(f'samples/{timestring}', exist_ok=True)\n",
        "      pil_image.save(f'samples/{timestring}/{j:04}.jpg')\n",
        "      j+=1\n",
        "\n",
        "try:\n",
        "  timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "  run(timestring)\n",
        "except KeyboardInterrupt:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestring"
      ],
      "metadata": {
        "id": "fcsTBrnSC_2O",
        "outputId": "733f4ecc-6323-4569-e3d0-0f68bd1d1dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'20220727000419'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**Generate video** ðŸŽ¥\n",
        "\n",
        "#@markdown You can edit frame rate and stuff by double-clicking this tab.\n",
        "\n",
        "frames = os.listdir(f\"samples/{timestring}\")\n",
        "frames = len(list(filter(lambda filename: filename.endswith(\".jpg\"), frames))) #Get number of jpg generated\n",
        "\n",
        "init_frame = 1 #This is the frame where the video will start\n",
        "last_frame = frames #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "\n",
        "min_fps = 10\n",
        "max_fps = 60\n",
        "\n",
        "total_frames = last_frame-init_frame\n",
        "\n",
        "#Desired video time in seconds\n",
        "video_length =  12#@param {type:\"number\"}\n",
        "#Video filename\n",
        "video_name = \"caffeinated_saveevery2a\" #@param {type:\"string\"}\n",
        "video_name = slugify(video_name)\n",
        "\n",
        "if not video_name:\n",
        "  video_name = \"video\"\n",
        "# frames = []\n",
        "# tqdm.write('Generating video...')\n",
        "# for i in range(init_frame,last_frame): #\n",
        "#     filename = f\"samples/{timestring}/{i:04}.jpg\"\n",
        "#     frames.append(Image.open(filename))\n",
        "\n",
        "fps = np.clip(total_frames/video_length,min_fps,max_fps)\n",
        "\n",
        "print(\"Generating video...\")\n",
        "!ffmpeg -r {fps} -i samples/{timestring}/%04d.jpg -c:v libx264 -vf fps={fps} -pix_fmt yuv420p samples/{video_name}.mp4 -frames:v {total_frames}\n",
        "\n",
        "# from subprocess import Popen, PIPE\n",
        "# p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', f'samples/{video_name}.mp4'], stdin=PIPE)\n",
        "# for im in tqdm(frames):\n",
        "#     im.save(p.stdin, 'PNG')\n",
        "# p.stdin.close()\n",
        "\n",
        "print(\"The video is ready\")"
      ],
      "metadata": {
        "id": "XhD937BdKRO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c518d1-00a4-48cd-ecb5-b73e0fe37ffe"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating video...\n",
            "ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "\u001b[0;33mTrailing options were found on the commandline.\n",
            "\u001b[0mInput #0, image2, from 'samples/20220727000419/%04d.jpg':\n",
            "  Duration: 00:00:00.04, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1024x1024 [SAR 1:1 DAR 1:1], 25 tbr, 25 tbn, 25 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x55915fe46000] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0m\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0mprofile High, level 3.2\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=14 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'samples/caffeinated_saveevery2a.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1024x1024 [SAR 1:1 DAR 1:1], q=-1--1, 14.92 fps, 11456 tbn, 14.92 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=    1 fps=0.0 q=27.0 Lsize=      54kB time=00:00:00.00 bitrate=5083126.4kbits/s speed=0.0014x    \n",
            "video:53kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.498265%\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0mframe I:1     Avg QP:24.46  size: 53774\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0mmb I  I16..4: 10.9% 84.1%  4.9%\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0m8x8 transform intra:84.1%\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0mcoded y,uvDC,uvAC intra: 67.9% 86.5% 54.0%\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0mi16 v,h,dc,p: 42% 26%  5% 27%\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 25% 18%  6%  5%  4%  5%  6%  8%\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 20% 23%  8%  7% 11%  9% 10%  6%  6%\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0mi8c dc,h,v,p: 42% 28% 20% 10%\n",
            "\u001b[1;36m[libx264 @ 0x55915fc17e00] \u001b[0mkb/s:6417.03\n",
            "The video is ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**Download video** ðŸ“€\n",
        "#@markdown If you're activated the download to GDrive option, the video will be save there. Don't worry about overwritting issues for colliding filenames, an id will be added to them to avoid this.\n",
        "\n",
        "#Video filename\n",
        "to_download_video_name = \"caffeinated_saveevery2a\" #@param {type:\"string\"}\n",
        "to_download_video_name = slugify(to_download_video_name)\n",
        "\n",
        "if not to_download_video_name:\n",
        "  to_download_video_name = \"video\"\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "if os.path.isdir('drive/MyDrive/samples'):\n",
        "  filelist = glob.glob(f'drive/MyDrive/samples/{to_download_video_name}*.mp4')\n",
        "  video_count = len(filelist)\n",
        "  if video_count:\n",
        "    final_video_name = f\"{to_download_video_name}{video_count}\"\n",
        "  else:\n",
        "    final_video_name = to_download_video_name\n",
        "  shutil.copyfile(f'samples/{video_name}.mp4', f'drive/MyDrive/samples/{final_video_name}.mp4')\n",
        "else:\n",
        "  files.download(f\"samples/{to_download_video_name}.mp4\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2-I0zmhomm_q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f9b4afed-f0b6-420c-8517-ec8e4063d927"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8294a83f-0047-4e16-926a-2cfd15bb33ef\", \"caffeinated_saveevery2a.mp4\", 55279)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "StyleganXL+CLIP",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}